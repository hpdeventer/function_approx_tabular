{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b884ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from model_data_definitions import *\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "# Function to generate cross validation dataset\n",
    "def generate_cross_validation_dataset(data, num_folds):\n",
    "    X, y = data.drop('target', axis=1).values, data['target'].values\n",
    "    \n",
    "    dataset_list = []\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    fold = 0\n",
    "\n",
    "    # Splitting data into training and testing set for each fold in the cross-validation \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold += 1\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        dataset_list.append((X_train, y_train, X_test, y_test , fold))\n",
    "    \n",
    "    return dataset_list\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_evaluate_model(model_tuple, fold_data, epoch_number, dataset_name,num_folds):\n",
    "    \n",
    "    model, name = model_tuple\n",
    "    X_train, y_train, X_test, y_test , fold = fold_data\n",
    "    \n",
    "    # Training the model \n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epoch_number,\n",
    "                        verbose=0,\n",
    "                        validation_data=(X_test,y_test))\n",
    "\n",
    "     # Evaluating the trained model on test data \n",
    "    loss = model.evaluate(X_test,y_test)\n",
    "\n",
    "     # Making predictions on the test data \n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "     # Calculate metrics \n",
    "    r_squared_value=r2_score(y_true=y_test,y_pred=predictions)\n",
    "    test_error=mean_squared_error(y_true=y_test,y_pred=predictions)\n",
    "\n",
    "    results = {\n",
    "        'model': name,\n",
    "        'fold': fold,\n",
    "        'train_history': history.history['loss'],\n",
    "        'val_history': history.history['val_loss'],\n",
    "        'loss': loss,\n",
    "        'r_squared_value': r_squared_value,\n",
    "        'test_error': test_error}\n",
    "\n",
    "    # Save results to numpy file\n",
    "    if not os.path.exists('aggregate_results'):\n",
    "        os.makedirs('aggregate_results')\n",
    "\n",
    "    np.save(f'aggregate_results/{dataset_name}-{name}-fold-{fold}-of-{num_folds}.npy', results)\n",
    "\n",
    "# Function to evaluate models in parallel\n",
    "def evaluate_models_parallel(fold_data, dataset_name, epoch_number,num_folds):\n",
    "    \n",
    "    models = initialize_all_models(fold_data[0].shape[1], seed_val=fold_data[4])\n",
    "    compile_models(models)\n",
    "\n",
    "     # Training and evaluating all models in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(train_evaluate_model, model, fold_data, epoch_number, dataset_name,num_folds): model for model in models}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# Function to evaluate all folds in parallel\n",
    "def evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number,num_folds):\n",
    "    \n",
    "     # Evaluating all folds in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(evaluate_models_parallel, fold_data, dataset_name, epoch_number,num_folds): fold_data for fold_data in kfold_datasets}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# New function to retrieve all datasets and their names and feed it to relevant functions with a for loop.\n",
    "def retrieve_datasets_and_run_evaluations(num_folds=5, epoch_number=100):\n",
    "    # Fetching data \n",
    "    filtered_datasets_metadata, datasets = fetch_return_filtered_pmlb_data_sets()\n",
    "\n",
    "    for dataset, row in zip(datasets, filtered_datasets_metadata.iterrows()):\n",
    "        dataset_name = row[1]['dataset']\n",
    "        kfold_datasets = generate_cross_validation_dataset(dataset, num_folds)\n",
    "        evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number,num_folds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64eb7590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>n_instances</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_binary_features</th>\n",
       "      <th>n_categorical_features</th>\n",
       "      <th>n_continuous_features</th>\n",
       "      <th>endpoint_type</th>\n",
       "      <th>n_classes</th>\n",
       "      <th>imbalance</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>488</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.099363</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1029_LEV</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.111245</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1030_ERA</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1096_FacultySalaries</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>192_vineyard</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.040475</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>228_elusage</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>230_machine_cpu</td>\n",
       "      <td>209</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>continuous</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.004906</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>485_analcatdata_vehicle</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>519_vinnie</td>\n",
       "      <td>380</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.030146</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>523_analcatdata_neavote</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.136914</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>529_pollen</td>\n",
       "      <td>3848</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>3784.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>556_analcatdata_apnea2</td>\n",
       "      <td>475</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>continuous</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.272393</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>557_analcatdata_apnea1</td>\n",
       "      <td>475</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>continuous</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.325260</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>579_fri_c0_250_5</td>\n",
       "      <td>250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>594_fri_c2_100_5</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>596_fri_c2_250_5</td>\n",
       "      <td>250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>597_fri_c2_500_5</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>599_fri_c2_1000_5</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>601_fri_c1_250_5</td>\n",
       "      <td>250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>609_fri_c0_1000_5</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>611_fri_c3_100_5</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>612_fri_c1_1000_5</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>613_fri_c3_250_5</td>\n",
       "      <td>250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>617_fri_c3_500_5</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>624_fri_c0_100_5</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>628_fri_c3_1000_5</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>631_fri_c1_500_5</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>649_fri_c0_500_5</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>656_fri_c1_100_5</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>663_rabe_266</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>665_sleuth_case2002</td>\n",
       "      <td>147</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>continuous</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.096020</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>678_visualizing_environmental</td>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>continuous</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>687_sleuth_ex1605</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.006465</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>690_visualizing_galaxy</td>\n",
       "      <td>323</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>706_sleuth_case1202</td>\n",
       "      <td>93</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>continuous</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>712_chscase_geyser1</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.012844</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>banana</td>\n",
       "      <td>5300</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.010691</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>titanic</td>\n",
       "      <td>2201</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>continuous</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.125266</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           dataset  n_instances  n_features  \\\n",
       "0                         1027_ESL          488           4   \n",
       "2                         1029_LEV         1000           4   \n",
       "3                         1030_ERA         1000           4   \n",
       "5             1096_FacultySalaries           50           4   \n",
       "13                    192_vineyard           52           2   \n",
       "23                     228_elusage           55           2   \n",
       "25                 230_machine_cpu          209           6   \n",
       "29         485_analcatdata_vehicle           48           4   \n",
       "32                      519_vinnie          380           2   \n",
       "34         523_analcatdata_neavote          100           2   \n",
       "36                      529_pollen         3848           4   \n",
       "40          556_analcatdata_apnea2          475           3   \n",
       "41          557_analcatdata_apnea1          475           3   \n",
       "48                579_fri_c0_250_5          250           5   \n",
       "60                594_fri_c2_100_5          100           5   \n",
       "62                596_fri_c2_250_5          250           5   \n",
       "63                597_fri_c2_500_5          500           5   \n",
       "65               599_fri_c2_1000_5         1000           5   \n",
       "66                601_fri_c1_250_5          250           5   \n",
       "74               609_fri_c0_1000_5         1000           5   \n",
       "75                611_fri_c3_100_5          100           5   \n",
       "76               612_fri_c1_1000_5         1000           5   \n",
       "77                613_fri_c3_250_5          250           5   \n",
       "80                617_fri_c3_500_5          500           5   \n",
       "86                624_fri_c0_100_5          100           5   \n",
       "89               628_fri_c3_1000_5         1000           5   \n",
       "90                631_fri_c1_500_5          500           5   \n",
       "102               649_fri_c0_500_5          500           5   \n",
       "107               656_fri_c1_100_5          100           5   \n",
       "111                   663_rabe_266          120           2   \n",
       "112            665_sleuth_case2002          147           6   \n",
       "114  678_visualizing_environmental          111           3   \n",
       "115              687_sleuth_ex1605           62           5   \n",
       "116         690_visualizing_galaxy          323           4   \n",
       "118            706_sleuth_case1202           93           6   \n",
       "119            712_chscase_geyser1          222           2   \n",
       "155                         banana         5300           2   \n",
       "270                        titanic         2201           3   \n",
       "\n",
       "     n_binary_features  n_categorical_features  n_continuous_features  \\\n",
       "0                    0                       0                      4   \n",
       "2                    0                       0                      4   \n",
       "3                    0                       0                      4   \n",
       "5                    0                       0                      4   \n",
       "13                   0                       0                      2   \n",
       "23                   0                       0                      2   \n",
       "25                   0                       0                      6   \n",
       "29                   0                       0                      4   \n",
       "32                   0                       0                      2   \n",
       "34                   0                       0                      2   \n",
       "36                   0                       0                      4   \n",
       "40                   0                       0                      3   \n",
       "41                   0                       0                      3   \n",
       "48                   0                       0                      5   \n",
       "60                   0                       0                      5   \n",
       "62                   0                       0                      5   \n",
       "63                   0                       0                      5   \n",
       "65                   0                       0                      5   \n",
       "66                   0                       0                      5   \n",
       "74                   0                       0                      5   \n",
       "75                   0                       0                      5   \n",
       "76                   0                       0                      5   \n",
       "77                   0                       0                      5   \n",
       "80                   0                       0                      5   \n",
       "86                   0                       0                      5   \n",
       "89                   0                       0                      5   \n",
       "90                   0                       0                      5   \n",
       "102                  0                       0                      5   \n",
       "107                  0                       0                      5   \n",
       "111                  0                       0                      2   \n",
       "112                  0                       0                      6   \n",
       "114                  0                       0                      3   \n",
       "115                  0                       0                      5   \n",
       "116                  0                       0                      4   \n",
       "118                  0                       0                      6   \n",
       "119                  0                       0                      2   \n",
       "155                  0                       0                      2   \n",
       "270                  0                       0                      3   \n",
       "\n",
       "    endpoint_type  n_classes  imbalance        task  \n",
       "0      continuous        9.0   0.099363  regression  \n",
       "2      continuous        5.0   0.111245  regression  \n",
       "3      continuous        9.0   0.031251  regression  \n",
       "5      continuous       39.0   0.004063  regression  \n",
       "13     continuous       19.0   0.040475  regression  \n",
       "23     continuous       52.0   0.000953  regression  \n",
       "25     continuous      116.0   0.004906  regression  \n",
       "29     continuous       47.0   0.000434  regression  \n",
       "32     continuous       16.0   0.030146  regression  \n",
       "34     continuous        8.0   0.136914  regression  \n",
       "36     continuous     3784.0   0.000004  regression  \n",
       "40     continuous      178.0   0.272393  regression  \n",
       "41     continuous      164.0   0.325260  regression  \n",
       "48     continuous      250.0   0.000000  regression  \n",
       "60     continuous      100.0   0.000000  regression  \n",
       "62     continuous      250.0   0.000000  regression  \n",
       "63     continuous      500.0   0.000000  regression  \n",
       "65     continuous     1000.0   0.000000  regression  \n",
       "66     continuous      250.0   0.000000  regression  \n",
       "74     continuous     1000.0   0.000000  regression  \n",
       "75     continuous      100.0   0.000000  regression  \n",
       "76     continuous     1000.0   0.000000  regression  \n",
       "77     continuous      250.0   0.000000  regression  \n",
       "80     continuous      500.0   0.000000  regression  \n",
       "86     continuous      100.0   0.000000  regression  \n",
       "89     continuous     1000.0   0.000000  regression  \n",
       "90     continuous      500.0   0.000000  regression  \n",
       "102    continuous      500.0   0.000000  regression  \n",
       "107    continuous      100.0   0.000000  regression  \n",
       "111    continuous       96.0   0.001825  regression  \n",
       "112    continuous       19.0   0.096020  regression  \n",
       "114    continuous       28.0   0.019608  regression  \n",
       "115    continuous       41.0   0.006465  regression  \n",
       "116    continuous      208.0   0.001246  regression  \n",
       "118    continuous       79.0   0.001583  regression  \n",
       "119    continuous       50.0   0.012844  regression  \n",
       "155    continuous        2.0   0.010691  regression  \n",
       "270    continuous        2.0   0.125266  regression  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_datasets_metadata, datasets = fetch_return_filtered_pmlb_data_sets()\n",
    "filtered_datasets_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1606e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the new function \n",
    "start_time = time.time()\n",
    "retrieve_datasets_and_run_evaluations(num_folds=4, epoch_number=2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The experiment took {elapsed_time} seconds to complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1b67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from model_data_definitions import *\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "# Function to generate cross validation dataset\n",
    "def generate_cross_validation_dataset(data, num_folds):\n",
    "    X, y = data.drop('target', axis=1).values, data['target'].values\n",
    "    \n",
    "    dataset_list = []\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    fold = 0\n",
    "\n",
    "    # Splitting data into training and testing set for each fold in the cross-validation \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold += 1\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        X_train, X_test = preprocess_data(X_train, X_test)\n",
    "        \n",
    "        dataset_list.append((X_train, y_train, X_test, y_test , fold))\n",
    "    \n",
    "    return dataset_list\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_evaluate_model(model_tuple, fold_data, epoch_number, dataset_name,num_folds):\n",
    "    \n",
    "    model, name = model_tuple\n",
    "    X_train, y_train, X_test, y_test , fold = fold_data\n",
    "    \n",
    "    # Training the model \n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epoch_number,\n",
    "                        verbose=0,\n",
    "                        validation_data=(X_test,y_test))\n",
    "\n",
    "     # Evaluating the trained model on test data \n",
    "    loss = model.evaluate(X_test,y_test)\n",
    "\n",
    "     # Making predictions on the test data \n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "     # Calculate metrics \n",
    "    r_squared_value=r2_score(y_true=y_test,y_pred=predictions)\n",
    "    test_error=mean_squared_error(y_true=y_test,y_pred=predictions)\n",
    "\n",
    "    results = {\n",
    "        'model': name,\n",
    "        'fold': fold,\n",
    "        'train_history': history.history['loss'],\n",
    "        'val_history': history.history['val_loss'],\n",
    "        'loss': loss,\n",
    "        'r_squared_value': r_squared_value,\n",
    "        'test_error': test_error}\n",
    "\n",
    "    # Save results to numpy file\n",
    "    if not os.path.exists('aggregate_results'):\n",
    "        os.makedirs('aggregate_results')\n",
    "\n",
    "    np.save(f'aggregate_results/{dataset_name}-{name}-epochs-{epoch_number}-fold-{fold}-of-{num_folds}.npy', results)\n",
    "\n",
    "# Function to evaluate models in parallel\n",
    "def evaluate_models_parallel(fold_data, dataset_name, epoch_number,num_folds):\n",
    "    \n",
    "    models = initialize_all_models(fold_data[0].shape[1], seed_val=fold_data[4])\n",
    "    compile_models(models)\n",
    "\n",
    "     # Training and evaluating all models in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(train_evaluate_model, \n",
    "                                   model, \n",
    "                                   fold_data, \n",
    "                                   epoch_number, \n",
    "                                   dataset_name,\n",
    "                                   num_folds): model for model in models}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# Function to evaluate all folds in parallel\n",
    "def evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number,num_folds):\n",
    "    \n",
    "     # Evaluating all folds in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(evaluate_models_parallel, \n",
    "                                   fold_data, \n",
    "                                   dataset_name, \n",
    "                                   epoch_number,\n",
    "                                   num_folds): fold_data for fold_data in kfold_datasets}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# New function to retrieve all datasets and their names and feed it to relevant functions with a for loop.\n",
    "def retrieve_datasets_and_run_evaluations(num_folds=5, epoch_number=100):\n",
    "    # Fetching data \n",
    "    filtered_datasets_metadata, datasets = fetch_return_filtered_pmlb_data_sets()\n",
    "\n",
    "    for dataset, row in zip(datasets, filtered_datasets_metadata.iterrows()):\n",
    "        dataset_name = row[1]['dataset']\n",
    "        kfold_datasets = generate_cross_validation_dataset(dataset, num_folds)\n",
    "        evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number,num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebaeb829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>n_instances</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_binary_features</th>\n",
       "      <th>n_categorical_features</th>\n",
       "      <th>n_continuous_features</th>\n",
       "      <th>endpoint_type</th>\n",
       "      <th>n_classes</th>\n",
       "      <th>imbalance</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>488</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.099363</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1029_LEV</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.111245</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1030_ERA</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1096_FacultySalaries</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>192_vineyard</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.040475</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>228_elusage</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>230_machine_cpu</td>\n",
       "      <td>209</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>continuous</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.004906</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>485_analcatdata_vehicle</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>519_vinnie</td>\n",
       "      <td>380</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.030146</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>523_analcatdata_neavote</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.136914</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>529_pollen</td>\n",
       "      <td>3848</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>3784.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>556_analcatdata_apnea2</td>\n",
       "      <td>475</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>continuous</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.272393</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>557_analcatdata_apnea1</td>\n",
       "      <td>475</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>continuous</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.325260</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>579_fri_c0_250_5</td>\n",
       "      <td>250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>594_fri_c2_100_5</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>596_fri_c2_250_5</td>\n",
       "      <td>250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>597_fri_c2_500_5</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>599_fri_c2_1000_5</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>601_fri_c1_250_5</td>\n",
       "      <td>250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>609_fri_c0_1000_5</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>611_fri_c3_100_5</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>612_fri_c1_1000_5</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>613_fri_c3_250_5</td>\n",
       "      <td>250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>617_fri_c3_500_5</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>624_fri_c0_100_5</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>628_fri_c3_1000_5</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>631_fri_c1_500_5</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>649_fri_c0_500_5</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>656_fri_c1_100_5</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>663_rabe_266</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>665_sleuth_case2002</td>\n",
       "      <td>147</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>continuous</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.096020</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>678_visualizing_environmental</td>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>continuous</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>687_sleuth_ex1605</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>continuous</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.006465</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>690_visualizing_galaxy</td>\n",
       "      <td>323</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>continuous</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>706_sleuth_case1202</td>\n",
       "      <td>93</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>continuous</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>712_chscase_geyser1</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.012844</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>banana</td>\n",
       "      <td>5300</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>continuous</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.010691</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>titanic</td>\n",
       "      <td>2201</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>continuous</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.125266</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           dataset  n_instances  n_features  \\\n",
       "0                         1027_ESL          488           4   \n",
       "2                         1029_LEV         1000           4   \n",
       "3                         1030_ERA         1000           4   \n",
       "5             1096_FacultySalaries           50           4   \n",
       "13                    192_vineyard           52           2   \n",
       "23                     228_elusage           55           2   \n",
       "25                 230_machine_cpu          209           6   \n",
       "29         485_analcatdata_vehicle           48           4   \n",
       "32                      519_vinnie          380           2   \n",
       "34         523_analcatdata_neavote          100           2   \n",
       "36                      529_pollen         3848           4   \n",
       "40          556_analcatdata_apnea2          475           3   \n",
       "41          557_analcatdata_apnea1          475           3   \n",
       "48                579_fri_c0_250_5          250           5   \n",
       "60                594_fri_c2_100_5          100           5   \n",
       "62                596_fri_c2_250_5          250           5   \n",
       "63                597_fri_c2_500_5          500           5   \n",
       "65               599_fri_c2_1000_5         1000           5   \n",
       "66                601_fri_c1_250_5          250           5   \n",
       "74               609_fri_c0_1000_5         1000           5   \n",
       "75                611_fri_c3_100_5          100           5   \n",
       "76               612_fri_c1_1000_5         1000           5   \n",
       "77                613_fri_c3_250_5          250           5   \n",
       "80                617_fri_c3_500_5          500           5   \n",
       "86                624_fri_c0_100_5          100           5   \n",
       "89               628_fri_c3_1000_5         1000           5   \n",
       "90                631_fri_c1_500_5          500           5   \n",
       "102               649_fri_c0_500_5          500           5   \n",
       "107               656_fri_c1_100_5          100           5   \n",
       "111                   663_rabe_266          120           2   \n",
       "112            665_sleuth_case2002          147           6   \n",
       "114  678_visualizing_environmental          111           3   \n",
       "115              687_sleuth_ex1605           62           5   \n",
       "116         690_visualizing_galaxy          323           4   \n",
       "118            706_sleuth_case1202           93           6   \n",
       "119            712_chscase_geyser1          222           2   \n",
       "155                         banana         5300           2   \n",
       "270                        titanic         2201           3   \n",
       "\n",
       "     n_binary_features  n_categorical_features  n_continuous_features  \\\n",
       "0                    0                       0                      4   \n",
       "2                    0                       0                      4   \n",
       "3                    0                       0                      4   \n",
       "5                    0                       0                      4   \n",
       "13                   0                       0                      2   \n",
       "23                   0                       0                      2   \n",
       "25                   0                       0                      6   \n",
       "29                   0                       0                      4   \n",
       "32                   0                       0                      2   \n",
       "34                   0                       0                      2   \n",
       "36                   0                       0                      4   \n",
       "40                   0                       0                      3   \n",
       "41                   0                       0                      3   \n",
       "48                   0                       0                      5   \n",
       "60                   0                       0                      5   \n",
       "62                   0                       0                      5   \n",
       "63                   0                       0                      5   \n",
       "65                   0                       0                      5   \n",
       "66                   0                       0                      5   \n",
       "74                   0                       0                      5   \n",
       "75                   0                       0                      5   \n",
       "76                   0                       0                      5   \n",
       "77                   0                       0                      5   \n",
       "80                   0                       0                      5   \n",
       "86                   0                       0                      5   \n",
       "89                   0                       0                      5   \n",
       "90                   0                       0                      5   \n",
       "102                  0                       0                      5   \n",
       "107                  0                       0                      5   \n",
       "111                  0                       0                      2   \n",
       "112                  0                       0                      6   \n",
       "114                  0                       0                      3   \n",
       "115                  0                       0                      5   \n",
       "116                  0                       0                      4   \n",
       "118                  0                       0                      6   \n",
       "119                  0                       0                      2   \n",
       "155                  0                       0                      2   \n",
       "270                  0                       0                      3   \n",
       "\n",
       "    endpoint_type  n_classes  imbalance        task  \n",
       "0      continuous        9.0   0.099363  regression  \n",
       "2      continuous        5.0   0.111245  regression  \n",
       "3      continuous        9.0   0.031251  regression  \n",
       "5      continuous       39.0   0.004063  regression  \n",
       "13     continuous       19.0   0.040475  regression  \n",
       "23     continuous       52.0   0.000953  regression  \n",
       "25     continuous      116.0   0.004906  regression  \n",
       "29     continuous       47.0   0.000434  regression  \n",
       "32     continuous       16.0   0.030146  regression  \n",
       "34     continuous        8.0   0.136914  regression  \n",
       "36     continuous     3784.0   0.000004  regression  \n",
       "40     continuous      178.0   0.272393  regression  \n",
       "41     continuous      164.0   0.325260  regression  \n",
       "48     continuous      250.0   0.000000  regression  \n",
       "60     continuous      100.0   0.000000  regression  \n",
       "62     continuous      250.0   0.000000  regression  \n",
       "63     continuous      500.0   0.000000  regression  \n",
       "65     continuous     1000.0   0.000000  regression  \n",
       "66     continuous      250.0   0.000000  regression  \n",
       "74     continuous     1000.0   0.000000  regression  \n",
       "75     continuous      100.0   0.000000  regression  \n",
       "76     continuous     1000.0   0.000000  regression  \n",
       "77     continuous      250.0   0.000000  regression  \n",
       "80     continuous      500.0   0.000000  regression  \n",
       "86     continuous      100.0   0.000000  regression  \n",
       "89     continuous     1000.0   0.000000  regression  \n",
       "90     continuous      500.0   0.000000  regression  \n",
       "102    continuous      500.0   0.000000  regression  \n",
       "107    continuous      100.0   0.000000  regression  \n",
       "111    continuous       96.0   0.001825  regression  \n",
       "112    continuous       19.0   0.096020  regression  \n",
       "114    continuous       28.0   0.019608  regression  \n",
       "115    continuous       41.0   0.006465  regression  \n",
       "116    continuous      208.0   0.001246  regression  \n",
       "118    continuous       79.0   0.001583  regression  \n",
       "119    continuous       50.0   0.012844  regression  \n",
       "155    continuous        2.0   0.010691  regression  \n",
       "270    continuous        2.0   0.125266  regression  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_datasets_metadata, datasets = fetch_return_filtered_pmlb_data_sets()\n",
    "filtered_datasets_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc29032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 29ms/step - loss: 1.7063\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 13.8667\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.0412\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.4030\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 6.2668\n",
      "8/8 [==============================] - 1s 10ms/step\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.1602\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 6.1602\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.1602\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 6.1602\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 6.2668\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 6.2668\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 6.2668\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 6.2668\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 6.2668\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 6.1602\n",
      "8/8 [==============================] - 1s 14ms/stepss: 5.76\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 6.1602\n",
      "8/8 [==============================] - 1s 16ms/step\n",
      "8/8 [==============================] - 1s 18ms/step\n",
      "8/8 [==============================] - 1s 12ms/step\n",
      "8/8 [==============================] - 1s 14ms/step\n",
      "8/8 [==============================] - 1s 13ms/step\n",
      "8/8 [==============================] - 1s 21ms/step\n",
      "8/8 [==============================] - 1s 15ms/step\n",
      "8/8 [==============================] - 1s 14ms/step\n",
      "8/8 [==============================] - 1s 15ms/step\n",
      "8/8 [==============================] - 1s 13ms/step\n",
      "8/8 [==============================] - 1s 18ms/step\n",
      "8/8 [==============================] - 1s 10ms/step\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5678\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.3424\n",
      "8/8 [==============================] - 1s 11ms/step\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 5.0863\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 5.2193\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 5.2118\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 5.2445\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 5.1070\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 5.2252\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 5.2424\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 5.1088\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 5.1299\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 5.1229\n",
      "8/8 [==============================] - 2s 6ms/step\n",
      "8/8 [==============================] - 2s 5ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 3ms/step\n",
      "8/8 [==============================] - 2s 3ms/step\n",
      "8/8 [==============================] - 2s 5ms/step\n",
      "8/8 [==============================] - 2s 5ms/step\n",
      "8/8 [==============================] - 2s 2ms/step\n",
      "8/8 [==============================] - 2s 2ms/step\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 5.0719\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 5.0068\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.8996\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 4.9433\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.0667\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4.9757\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 5.0452\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.9477\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.0603\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.8874\n",
      "8/8 [==============================] - 2s 6ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 5ms/step\n",
      "8/8 [==============================] - 2s 5ms/step\n",
      "8/8 [==============================] - 2s 5ms/step\n",
      "8/8 [==============================] - 2s 5ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 1ms/step\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.9011\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3.5270\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5122\n",
      "16/16 [==============================] - 1s 11ms/stepss: 2.56\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.5603\n",
      "16/16 [==============================] - 1s 14ms/stepss: 0.51\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5115\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 2.7500\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 2.5985\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 2.7500\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 2.7060\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.7560\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 2.5595\n",
      "16/16 [==============================] - 1s 16ms/stepss: 2.561\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 2.5961\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 2.5541\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.7560\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 2.5554\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 2.7087\n",
      "16/16 [==============================] - 1s 11ms/step\n",
      "16/16 [==============================] - 1s 16ms/step\n",
      "16/16 [==============================] - 1s 14ms/step\n",
      "16/16 [==============================] - 1s 14ms/step\n",
      "16/16 [==============================] - 1s 13ms/step\n",
      "16/16 [==============================] - 1s 14ms/step\n",
      "16/16 [==============================] - 1s 19ms/step\n",
      "16/16 [==============================] - 1s 20ms/step\n",
      "16/16 [==============================] - 1s 18ms/step\n",
      "16/16 [==============================] - 1s 30ms/step\n",
      "16/16 [==============================] - 1s 13ms/step\n",
      "16/16 [==============================] - 1s 13ms/step\n",
      "16/16 [==============================] - 1s 11ms/step\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7111\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8207\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.6918\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 1.7103\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.6759\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.6825\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 1.6908\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.6799\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 1.6882\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 1.6924\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 1.6861\n",
      "16/16 [==============================] - 2s 6ms/step\n",
      "16/16 [==============================] - 2s 7ms/step\n",
      "16/16 [==============================] - 2s 6ms/step\n",
      "16/16 [==============================] - 2s 6ms/step\n",
      "16/16 [==============================] - 2s 7ms/step\n",
      "16/16 [==============================] - 2s 5ms/step\n",
      "16/16 [==============================] - 2s 5ms/step\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "16/16 [==============================] - 2s 4ms/step\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.4557\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.4432\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.3827\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.3596\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.3567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 12ms/step - loss: 1.4069\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.4246\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.4210\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.4272\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3638\n",
      "16/16 [==============================] - 2s 7ms/step\n",
      "16/16 [==============================] - 2s 6ms/step\n",
      "16/16 [==============================] - 2s 6ms/step\n",
      "16/16 [==============================] - 2s 8ms/step\n",
      "16/16 [==============================] - 2s 5ms/step\n",
      "16/16 [==============================] - 2s 8ms/step\n",
      "16/16 [==============================] - 2s 6ms/step\n",
      "16/16 [==============================] - 2s 6ms/step\n",
      "16/16 [==============================] - 2s 5ms/step\n",
      "16/16 [==============================] - 2s 5ms/step\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6623\n",
      "16/16 [==============================] - 0s 865us/step\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 7.7478\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 20.2350\n",
      "16/16 [==============================] - 1s 22ms/stepss: 2.\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1.9138\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 2.1159\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 4.9980\n",
      "16/16 [==============================] - 1s 29ms/step\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 4.9980\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 4.9980\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 4.9980\n",
      "16/16 [==============================] - 1s 30ms/step - loss: 4.9227\n",
      "16/16 [==============================] - 1s 31ms/step - loss: 5.2000\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 5.2000\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 5.1037\n",
      "16/16 [==============================] - 1s 30ms/step - loss: 5.2000\n",
      "16/16 [==============================] - 1s 31ms/step - loss: 4.9218\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 5.2000\n",
      "16/16 [==============================] - 1s 19ms/stepss: 5.1166\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 5.1017\n",
      "16/16 [==============================] - 1s 22ms/step\n",
      "16/16 [==============================] - 1s 18ms/step\n",
      "16/16 [==============================] - 1s 22ms/step\n",
      "16/16 [==============================] - 1s 23ms/step\n",
      "16/16 [==============================] - 1s 28ms/step\n",
      "16/16 [==============================] - 1s 18ms/step\n",
      "16/16 [==============================] - 1s 22ms/step\n",
      "16/16 [==============================] - 1s 21ms/step\n",
      "16/16 [==============================] - 1s 21ms/step\n",
      "16/16 [==============================] - 1s 18ms/step\n",
      "16/16 [==============================] - 1s 21ms/step\n",
      "16/16 [==============================] - 1s 13ms/step\n",
      "16/16 [==============================] - 1s 18ms/step\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.4783\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.8052\n",
      "16/16 [==============================] - 0s 4ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 4.1176\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 3.8736\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 3.8902\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 3.9074\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 4.1225\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 4.0998\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 3.9123\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 4.1190\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 3.9066\n",
      "16/16 [==============================] - 2s 4ms/step\n",
      "16/16 [==============================] - 2s 3ms/step\n",
      "16/16 [==============================] - 2s 6ms/step\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "16/16 [==============================] - 2s 8ms/step\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "16/16 [==============================] - 1s 2ms/step\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.7728\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.5564\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.5538\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.8082\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.5659\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.7136\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.5428\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.8350\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.7399\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.5867\n",
      "16/16 [==============================] - 2s 6ms/step\n",
      "16/16 [==============================] - 2s 7ms/step\n",
      "16/16 [==============================] - 2s 5ms/step\n",
      "16/16 [==============================] - 2s 8ms/step\n",
      "16/16 [==============================] - 2s 6ms/step\n",
      "16/16 [==============================] - 2s 7ms/step\n",
      "16/16 [==============================] - 2s 6ms/step\n",
      "16/16 [==============================] - 2s 5ms/step\n",
      "16/16 [==============================] - 2s 4ms/step\n",
      "16/16 [==============================] - 2s 4ms/step\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1081\n",
      "16/16 [==============================] - 0s 867us/step\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 114.7306\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 45.8625\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 31.6142\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 30.0686\n",
      "1/1 [==============================] - 0s 324ms/step\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 43.0540\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 43.0540\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 43.0540\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 45.4964\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 43.0540\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 45.4964\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 43.0540\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 45.4964\n",
      "1/1 [==============================] - 0s 401ms/step\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 45.4964\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 45.4964\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 45.4964\n",
      "1/1 [==============================] - 0s 365ms/step\n",
      "1/1 [==============================] - 0s 395ms/step\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 467ms/step\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "1/1 [==============================] - 0s 490ms/step\n",
      "1/1 [==============================] - 1s 590ms/step\n",
      "1/1 [==============================] - 1s 571ms/step\n",
      "1/1 [==============================] - 0s 498ms/step\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 1s 543ms/step\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 40.7437\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.8654\n",
      "1/1 [==============================] - 1s 541ms/step\n",
      "1/1 [==============================] - 0s 387ms/step\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 42.0271\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 42.0247\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 44.4653\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 42.0401\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 42.0418\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 42.0170\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 44.4875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step - loss: 44.4889\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 44.5198\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 44.4732\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 42.0783\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 44.4210\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 42.0029\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 42.0353\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 44.4728\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 44.4303\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 42.0387\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 44.4393\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 42.0412\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 44.4874\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 43.0540\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 14.1555\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.4239\n",
      "1/1 [==============================] - 1s 556ms/step\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 11.6050\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 11.6268\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 21.6711\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 21.6711\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 21.6711\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 16.4980\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 21.6711\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 16.4980\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 21.6711\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 16.4980\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 16.4980\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 21.6711\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 478ms/steps: 16.49\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 16.4980\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 16.4980\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 0s 449ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 1s 703ms/step\n",
      "1/1 [==============================] - 1s 553ms/step\n",
      "1/1 [==============================] - 1s 548ms/step\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "1/1 [==============================] - 1s 587ms/step\n",
      "1/1 [==============================] - 1s 553ms/step\n",
      "1/1 [==============================] - 1s 501ms/step\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 18.4659\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 20.6843\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 20.6808\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 20.6510\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 15.4963\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 20.6890\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 20.6661\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 15.4707\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 15.4828\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 15.4786\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 15.4822\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 20.6606\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 15.4669\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 20.6729\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 20.6434\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 15.4952\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 20.6493\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 15.5116\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 15.4995\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 20.6395\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 15.5017\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 15.4372\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 83.6814\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 56.1072\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 60.1225\n",
      "1/1 [==============================] - 1s 511ms/step\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 53.5860\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 39.7864\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 39.7864\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 39.7864\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 39.7864\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 39.7864\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 39.7864\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 48.9260\n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 48.9260\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 48.9260\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 48.9260\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 48.9260\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 48.9260\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "1/1 [==============================] - 1s 509ms/step\n",
      "1/1 [==============================] - 0s 491ms/step\n",
      "1/1 [==============================] - 0s 399ms/step\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "1/1 [==============================] - 1s 537ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 569ms/step\n",
      "1/1 [==============================] - 1s 507ms/step\n",
      "1/1 [==============================] - 0s 497ms/step\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 46.8380\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 22.4584\n",
      "1/1 [==============================] - 0s 409ms/step\n",
      "1/1 [==============================] - 0s 402ms/step\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 38.8044\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 38.8024\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 38.7962\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 47.9036\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 38.7842\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 47.9262\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 47.9214\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 47.9101\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 47.9231\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 38.7639\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 38.7671\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 47.9393\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 47.9273\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 38.7681\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 38.7945\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 38.7416\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 47.9178\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 47.9062\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 47.9256\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 38.7866\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 11250.5098\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 3697.6038\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 452.0959\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 557.4502\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 108.1747\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 105.0682\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 105.0682\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 105.0682\n",
      "4/4 [==============================] - 0s 12ms/stepss: 175.8047\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 108.1747\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 105.0682\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 108.1747\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 108.1747\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 108.1747\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 108.1747\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 105.0682\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 105.0682\n",
      "4/4 [==============================] - 1s 15ms/step\n",
      "4/4 [==============================] - 1s 8ms/step\n",
      "4/4 [==============================] - 1s 10ms/step\n",
      "4/4 [==============================] - 1s 14ms/step\n",
      "4/4 [==============================] - 1s 12ms/step\n",
      "4/4 [==============================] - 1s 12ms/step\n",
      "4/4 [==============================] - 1s 12ms/step\n",
      "4/4 [==============================] - 1s 11ms/step\n",
      "4/4 [==============================] - 1s 14ms/step\n",
      "4/4 [==============================] - 1s 12ms/step\n",
      "4/4 [==============================] - 1s 25ms/step\n",
      "4/4 [==============================] - 1s 16ms/step\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1397.6746\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 43.4352\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 107.1194\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 107.1110\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 107.1367\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 104.0209\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 107.1338\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 104.0346\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 104.0546\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 107.1120\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 104.0219\n",
      "4/4 [==============================] - 2s 3ms/step\n",
      "4/4 [==============================] - 2s 4ms/step\n",
      "4/4 [==============================] - 2s 4ms/step\n",
      "4/4 [==============================] - 2s 5ms/step\n",
      "4/4 [==============================] - 2s 9ms/step\n",
      "4/4 [==============================] - 2s 8ms/step\n",
      "4/4 [==============================] - 2s 8ms/step\n",
      "4/4 [==============================] - 2s 6ms/step\n",
      "4/4 [==============================] - 2s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 106.9681\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 107.0017\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 103.9139\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 103.9311\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 107.0201\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 103.9063\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 107.0256\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 103.8805\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 106.9985\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 103.9077\n",
      "4/4 [==============================] - 2s 7ms/step\n",
      "4/4 [==============================] - 2s 6ms/step\n",
      "4/4 [==============================] - 2s 5ms/step\n",
      "4/4 [==============================] - 2s 2ms/step\n",
      "4/4 [==============================] - 2s 3ms/step\n",
      "4/4 [==============================] - 2s 4ms/step\n",
      "4/4 [==============================] - 2s 2ms/step\n",
      "4/4 [==============================] - 2s 2ms/step\n",
      "4/4 [==============================] - 2s 2ms/step\n",
      "4/4 [==============================] - 2s 1ms/step\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 104.0358\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 350.5618\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 344.1095\n",
      "1/1 [==============================] - 0s 290ms/step\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 345.4147\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 349.6234\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 345.4147\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 345.4147\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 345.4147\n",
      "1/1 [==============================] - 0s 421ms/step\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 345.4147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step - loss: 350.3756\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 350.6632\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 345.4147\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 350.6632\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 350.3749\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 350.3723\n",
      "1/1 [==============================] - 1s 548ms/steps: 350.37\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 350.3766\n",
      "1/1 [==============================] - 1s 513ms/step\n",
      "1/1 [==============================] - 1s 560ms/step\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 466ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 1s 507ms/step\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "1/1 [==============================] - 1s 558ms/step\n",
      "1/1 [==============================] - 1s 540ms/step\n",
      "1/1 [==============================] - 0s 446ms/step\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 1s 592ms/step\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 344.3838\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 348.5410\n",
      "1/1 [==============================] - 0s 387ms/step\n",
      "1/1 [==============================] - 0s 373ms/step\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 344.4106\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 344.3438\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 344.4268\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 344.4027\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 344.3794\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 349.7328\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 349.7103\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 349.6866\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 349.7302\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 349.6945\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 349.7000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 344.3477\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 344.4264\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 344.5182\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 344.4361\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 349.6675\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 349.7599\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 349.6414\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 349.8062\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 344.3803\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 342.3685\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 153.3664\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 89.4492\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 78.8635\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 2.5197\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 9.7550\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 170.5198\n",
      "6/6 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Heinrich\\AppData\\Local\\Temp\\__autograph_generated_filebpvx0xla.py\", line 18, in tf__call\n        scaled_input = ag__.converted_call(ag__.ld(tf).floor, (ag__.ld(inputs) * ag__.ld(self).partition_num,), None, fscope)\n\n    TypeError: Exception encountered when calling layer \"lookup_table_model\" \"                 f\"(type LookupTableModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\Heinrich\\Documents\\GitHub\\function_approx_tabular\\model_data_definitions.py\", line 217, in call  *\n            scaled_input = tf.floor(inputs * self.partition_num)\n    \n        TypeError: Value passed to parameter 'x' has DataType int64 not in list of allowed values: bfloat16, float16, float32, float64\n    \n    \n    Call arguments received by layer \"lookup_table_model\" \"                 f\"(type LookupTableModel):\n       inputs=tf.Tensor(shape=(None, 2), dtype=int64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call the new function \u001b[39;00m\n\u001b[0;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mretrieve_datasets_and_run_evaluations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      5\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[4], line 105\u001b[0m, in \u001b[0;36mretrieve_datasets_and_run_evaluations\u001b[1;34m(num_folds, epoch_number)\u001b[0m\n\u001b[0;32m    103\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    104\u001b[0m kfold_datasets \u001b[38;5;241m=\u001b[39m generate_cross_validation_dataset(dataset, num_folds)\n\u001b[1;32m--> 105\u001b[0m \u001b[43mevaluate_all_folds_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkfold_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 95\u001b[0m, in \u001b[0;36mevaluate_all_folds_parallel\u001b[1;34m(kfold_datasets, dataset_name, epoch_number, num_folds)\u001b[0m\n\u001b[0;32m     89\u001b[0m futures \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(evaluate_models_parallel, \n\u001b[0;32m     90\u001b[0m                            fold_data, \n\u001b[0;32m     91\u001b[0m                            dataset_name, \n\u001b[0;32m     92\u001b[0m                            epoch_number,\n\u001b[0;32m     93\u001b[0m                            num_folds): fold_data \u001b[38;5;28;01mfor\u001b[39;00m fold_data \u001b[38;5;129;01min\u001b[39;00m kfold_datasets}\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\concurrent\\futures\\_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[4], line 82\u001b[0m, in \u001b[0;36mevaluate_models_parallel\u001b[1;34m(fold_data, dataset_name, epoch_number, num_folds)\u001b[0m\n\u001b[0;32m     75\u001b[0m futures \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(train_evaluate_model, \n\u001b[0;32m     76\u001b[0m                            model, \n\u001b[0;32m     77\u001b[0m                            fold_data, \n\u001b[0;32m     78\u001b[0m                            epoch_number, \n\u001b[0;32m     79\u001b[0m                            dataset_name,\n\u001b[0;32m     80\u001b[0m                            num_folds): model \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models}\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[1;32m---> 82\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m, in \u001b[0;36mtrain_evaluate_model\u001b[1;34m(model_tuple, fold_data, epoch_number, dataset_name, num_folds)\u001b[0m\n\u001b[0;32m     33\u001b[0m X_train, y_train, X_test, y_test , fold \u001b[38;5;241m=\u001b[39m fold_data\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Training the model \u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m  \u001b[38;5;66;03m# Evaluating the trained model on test data \u001b[39;00m\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test,y_test)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filew79tjps5.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filebpvx0xla.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 18\u001b[0m scaled_input \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m indices \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mld(scaled_input) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mpartition_num_powers,), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[0;32m     20\u001b[0m indices \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(indices),), \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mint32), fscope)\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Heinrich\\AppData\\Local\\Temp\\__autograph_generated_filebpvx0xla.py\", line 18, in tf__call\n        scaled_input = ag__.converted_call(ag__.ld(tf).floor, (ag__.ld(inputs) * ag__.ld(self).partition_num,), None, fscope)\n\n    TypeError: Exception encountered when calling layer \"lookup_table_model\" \"                 f\"(type LookupTableModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\Heinrich\\Documents\\GitHub\\function_approx_tabular\\model_data_definitions.py\", line 217, in call  *\n            scaled_input = tf.floor(inputs * self.partition_num)\n    \n        TypeError: Value passed to parameter 'x' has DataType int64 not in list of allowed values: bfloat16, float16, float32, float64\n    \n    \n    Call arguments received by layer \"lookup_table_model\" \"                 f\"(type LookupTableModel):\n       inputs=tf.Tensor(shape=(None, 2), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Call the new function \n",
    "start_time = time.time()\n",
    "retrieve_datasets_and_run_evaluations(num_folds=2, epoch_number=2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The experiment took {elapsed_time} seconds to complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684aa6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
