{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a7daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 15ms/step\n",
      "8/8 [==============================] - 1s 42ms/step\n",
      "8/8 [==============================] - 1s 33ms/step\n",
      "8/8 [==============================] - 1s 52ms/step\n",
      "8/8 [==============================] - 1s 38ms/step\n",
      "8/8 [==============================] - 1s 21ms/step\n",
      "8/8 [==============================] - 2s 53ms/step\n",
      "8/8 [==============================] - 2s 69ms/step\n",
      "8/8 [==============================] - 2s 51ms/step\n",
      "8/8 [==============================] - 2s 60ms/step\n",
      "8/8 [==============================] - 2s 41ms/step\n",
      "8/8 [==============================] - 2s 62ms/step\n",
      "8/8 [==============================] - 1s 41ms/step\n",
      "8/8 [==============================] - 2s 67ms/step\n",
      "1/8 [==>...........................] - ETA: 8sWARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A2D96C55A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "8/8 [==============================] - 1s 63ms/step\n",
      "1/8 [==>...........................] - ETA: 9sWARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A2D99DB130> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "8/8 [==============================] - 2s 110ms/step\n",
      "8/8 [==============================] - 2s 96ms/step\n",
      "8/8 [==============================] - 2s 153ms/step\n",
      "8/8 [==============================] - 2s 150ms/step\n",
      "8/8 [==============================] - 2s 155ms/step\n",
      "8/8 [==============================] - 2s 149ms/step\n",
      "8/8 [==============================] - 2s 140ms/step\n",
      "8/8 [==============================] - 2s 136ms/step\n",
      "8/8 [==============================] - 2s 141ms/step\n",
      "8/8 [==============================] - 2s 137ms/step\n",
      "8/8 [==============================] - 2s 136ms/step\n",
      "8/8 [==============================] - 2s 108ms/step\n",
      "8/8 [==============================] - 2s 114ms/step\n",
      "8/8 [==============================] - 2s 139ms/step\n",
      "8/8 [==============================] - 2s 125ms/step\n",
      "8/8 [==============================] - 2s 119ms/step\n",
      "8/8 [==============================] - 1s 21ms/step\n",
      "8/8 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 1s 5ms/step\n",
      "8/8 [==============================] - 6s 6ms/step\n",
      "8/8 [==============================] - 5s 5ms/step\n",
      "8/8 [==============================] - 5s 5ms/step\n",
      "8/8 [==============================] - 5s 9ms/step\n",
      "8/8 [==============================] - 4s 9ms/step\n",
      "8/8 [==============================] - 5s 11ms/step\n",
      "8/8 [==============================] - 5s 10ms/step\n",
      "8/8 [==============================] - 4s 7ms/step\n",
      "8/8 [==============================] - 5s 6ms/step\n",
      "8/8 [==============================] - 4s 10ms/step\n",
      "8/8 [==============================] - 4s 14ms/step\n",
      "8/8 [==============================] - 4s 10ms/step\n",
      "8/8 [==============================] - 4s 11ms/step\n",
      "8/8 [==============================] - 5s 6ms/step\n",
      "8/8 [==============================] - 4s 11ms/step\n",
      "8/8 [==============================] - 3s 10ms/step\n",
      "8/8 [==============================] - 4s 6ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 1s 4ms/step\n",
      "8/8 [==============================] - 4s 3ms/step\n",
      "8/8 [==============================] - 4s 3ms/step\n",
      "8/8 [==============================] - 4s 3ms/step\n",
      "8/8 [==============================] - 3s 18ms/step\n",
      "8/8 [==============================] - 3s 15ms/step\n",
      "8/8 [==============================] - 3s 15ms/step\n",
      "8/8 [==============================] - 3s 14ms/step\n",
      "8/8 [==============================] - 3s 7ms/step\n",
      "8/8 [==============================] - 3s 14ms/step\n",
      "8/8 [==============================] - 3s 16ms/step\n",
      "8/8 [==============================] - 3s 13ms/step\n",
      "8/8 [==============================] - 3s 17ms/step\n",
      "8/8 [==============================] - 3s 17ms/step\n",
      "8/8 [==============================] - 3s 15ms/step\n",
      "8/8 [==============================] - 3s 12ms/step\n",
      "8/8 [==============================] - 3s 11ms/step\n",
      "8/8 [==============================] - 3s 12ms/step\n",
      "8/8 [==============================] - 3s 8ms/step\n",
      "8/8 [==============================] - 3s 6ms/step\n",
      "8/8 [==============================] - 3s 5ms/step\n",
      "8/8 [==============================] - 1s 17ms/step\n",
      "8/8 [==============================] - 1s 23ms/step\n",
      "8/8 [==============================] - 1s 36ms/step\n",
      "8/8 [==============================] - 1s 25ms/step\n",
      "8/8 [==============================] - 1s 31ms/step\n",
      "8/8 [==============================] - 1s 15ms/step\n",
      "8/8 [==============================] - 1s 51ms/step\n",
      "8/8 [==============================] - 1s 35ms/step\n",
      "8/8 [==============================] - 1s 36ms/step\n",
      "8/8 [==============================] - 1s 36ms/step\n",
      "8/8 [==============================] - 1s 34ms/step\n",
      "8/8 [==============================] - 1s 14ms/step\n",
      "8/8 [==============================] - 1s 39ms/step\n",
      "8/8 [==============================] - 1s 48ms/step\n",
      "8/8 [==============================] - 1s 53ms/step\n",
      "8/8 [==============================] - 1s 62ms/step\n",
      "8/8 [==============================] - 1s 47ms/step\n",
      "8/8 [==============================] - 1s 48ms/step\n",
      "8/8 [==============================] - 1s 50ms/step\n",
      "8/8 [==============================] - 1s 26ms/step\n",
      "8/8 [==============================] - 1s 30ms/step\n",
      "8/8 [==============================] - 1s 22ms/step\n",
      "8/8 [==============================] - 1s 25ms/step\n",
      "8/8 [==============================] - 1s 17ms/step\n",
      "8/8 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 1s 4ms/step\n",
      "8/8 [==============================] - 1s 7ms/step\n",
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 3s 6ms/step\n",
      "8/8 [==============================] - 3s 8ms/step\n",
      "8/8 [==============================] - 3s 5ms/step\n",
      "8/8 [==============================] - 3s 5ms/step\n",
      "8/8 [==============================] - 3s 3ms/step\n",
      "8/8 [==============================] - 2s 9ms/step\n",
      "8/8 [==============================] - 2s 10ms/step\n",
      "8/8 [==============================] - 3s 6ms/step\n",
      "8/8 [==============================] - 2s 6ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 3ms/step\n",
      "8/8 [==============================] - 2s 3ms/step\n",
      "8/8 [==============================] - 3s 5ms/step\n",
      "8/8 [==============================] - 3s 9ms/step\n",
      "8/8 [==============================] - 3s 8ms/step\n",
      "8/8 [==============================] - 3s 7ms/step\n",
      "8/8 [==============================] - 3s 14ms/step\n",
      "8/8 [==============================] - 3s 10ms/step\n",
      "8/8 [==============================] - 3s 5ms/step\n",
      "8/8 [==============================] - 3s 10ms/step\n",
      "8/8 [==============================] - 3s 5ms/step\n",
      "8/8 [==============================] - 3s 5ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 4ms/step\n",
      "8/8 [==============================] - 3s 3ms/step\n",
      "8/8 [==============================] - 2s 2ms/step\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 1s 4ms/step\n",
      "8/8 [==============================] - 1s 5ms/step\n",
      "8/8 [==============================] - 1s 5ms/step\n",
      "8/8 [==============================] - 1s 5ms/step\n",
      "8/8 [==============================] - 1s 3ms/step\n",
      "8/8 [==============================] - 1s 3ms/step\n",
      "8/8 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 1s 4ms/step\n",
      "8/8 [==============================] - 1s 3ms/step\n",
      "8/8 [==============================] - 1s 4ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from model_data_definitions import *\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to generate cross validation dataset\n",
    "def generate_cross_validation_dataset(data, num_folds):\n",
    "    X, y = data.drop('target', axis=1).values, data['target'].values\n",
    "    \n",
    "    dataset_list = []\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    fold = 0\n",
    "\n",
    "    # Splitting data into training and testing set for each fold in the cross-validation \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold += 1\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        dataset_list.append((X_train, y_train, X_test, y_test , fold))\n",
    "    \n",
    "    return dataset_list\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_evaluate_model(model_tuple, fold_data, epoch_number, dataset_name):\n",
    "    \n",
    "    model, name = model_tuple\n",
    "    X_train, y_train, X_test, y_test , fold = fold_data\n",
    "    \n",
    "    # Training the model \n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epoch_number,\n",
    "                        verbose=0,\n",
    "                        validation_data=(X_test,y_test))\n",
    "\n",
    "     # Evaluating the trained model on test data\n",
    "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # Making predictions on the test data\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics \n",
    "    r_squared_value = r2_score(y_test,predictions)\n",
    "    test_error = mean_squared_error(y_test,predictions)\n",
    "\n",
    "    results = {\n",
    "        'model': name,\n",
    "        'fold': fold,\n",
    "        'train_history': history.history['loss'],\n",
    "        'val_history': history.history['val_loss'],\n",
    "        'loss': loss,\n",
    "        'r_squared_value': r_squared_value,\n",
    "        'test_error': test_error}\n",
    "\n",
    "    # Save results to numpy file\n",
    "    if not os.path.exists('aggregate_results'):\n",
    "        os.makedirs('aggregate_results')\n",
    "\n",
    "    np.save(f'aggregate_results/{dataset_name}-{name}-fold{fold}.npy', results)\n",
    "\n",
    "# Function to evaluate models in parallel\n",
    "def evaluate_models_parallel(fold_data, dataset_name):\n",
    "    \n",
    "    models = initialize_all_models(fold_data[0].shape[1], seed_val=fold_data[4])\n",
    "    compile_models(models)\n",
    "\n",
    "     # Training and evaluating all models in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(train_evaluate_model, model, fold_data, 2, dataset_name): model for model in models}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# Function to evaluate all folds in parallel\n",
    "def evaluate_all_folds_parallel(kfold_datasets, dataset_name):\n",
    "    \n",
    "     # Evaluating all folds in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(evaluate_models_parallel, fold_data, dataset_name): fold_data for fold_data in kfold_datasets}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# New function to retrieve all datasets and their names and feed it to relevant functions with a for loop.\n",
    "def retrieve_datasets_and_run_evaluations():\n",
    "    # Fetching data \n",
    "    filtered_datasets_metadata, datasets = fetch_return_filtered_pmlb_data_sets()\n",
    "\n",
    "    num_folds = 4\n",
    "\n",
    "    for dataset, row in zip(datasets, filtered_datasets_metadata.iterrows()):\n",
    "        dataset_name = row[1]['dataset']\n",
    "        kfold_datasets = generate_cross_validation_dataset(dataset, num_folds)\n",
    "        evaluate_all_folds_parallel(kfold_datasets, dataset_name)\n",
    "\n",
    "# Call the new function \n",
    "retrieve_datasets_and_run_evaluations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb092ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 52ms/step - loss: 3.2038\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.8088\n",
      "8/8 [==============================] - 1s 14ms/stepss: 3.52\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 3.5253\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.5238\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.4837\n",
      "8/8 [==============================] - 1s 22ms/step\n",
      "8/8 [==============================] - 1s 28ms/step\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.8040\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 2.8040\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.5883\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 2.7360\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.6342\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6098\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.5887\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.5408\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 2.5704\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.6755\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.7583\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 2.5714\n",
      "8/8 [==============================] - 1s 30ms/step\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 2.7320\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 2.7360\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 2.5598\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.7063\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 2.5233\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 2.7320\n",
      "8/8 [==============================] - 1s 73ms/stepss: 2.52\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.5219\n",
      "8/8 [==============================] - 1s 24ms/step\n",
      "8/8 [==============================] - 1s 39ms/step\n",
      "8/8 [==============================] - 1s 21ms/step\n",
      "8/8 [==============================] - 1s 39ms/step\n",
      "8/8 [==============================] - 1s 34ms/step\n",
      "8/8 [==============================] - 1s 28ms/step\n",
      "8/8 [==============================] - 1s 21ms/step\n",
      "8/8 [==============================] - 1s 33ms/step\n",
      "8/8 [==============================] - 1s 20ms/step\n",
      "8/8 [==============================] - 1s 31ms/step\n",
      "8/8 [==============================] - 1s 33ms/step\n",
      "8/8 [==============================] - 1s 31ms/step\n",
      "8/8 [==============================] - 1s 14ms/step\n",
      "8/8 [==============================] - 1s 15ms/step\n",
      "8/8 [==============================] - 1s 17ms/step\n",
      "8/8 [==============================] - 1s 19ms/step\n",
      "8/8 [==============================] - 1s 30ms/step\n",
      "8/8 [==============================] - 1s 33ms/step\n",
      "8/8 [==============================] - 1s 32ms/step\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5666\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5514\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6595\n",
      "8/8 [==============================] - 1s 5ms/step\n",
      "8/8 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 1s 4ms/step\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.6956\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.6376\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.6280\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.7099\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 1.6381\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.6961\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.6124\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.6397\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.6341\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.6569\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.6830\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6274\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.6407\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.6884\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.6407\n",
      "8/8 [==============================] - 4s 8ms/step\n",
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 4s 4ms/step\n",
      "8/8 [==============================] - 3s 5ms/step\n",
      "8/8 [==============================] - 3s 4ms/step\n",
      "8/8 [==============================] - 3s 4ms/step\n",
      "8/8 [==============================] - 3s 4ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 2s 6ms/step\n",
      "8/8 [==============================] - 2s 14ms/step\n",
      "8/8 [==============================] - 2s 14ms/step\n",
      "8/8 [==============================] - 2s 10ms/step\n",
      "8/8 [==============================] - 2s 10ms/step\n",
      "8/8 [==============================] - 2s 6ms/step\n",
      "8/8 [==============================] - 2s 6ms/step\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 1.2318\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.2504\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.1679\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.1925\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.2618\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.2103\n",
      "8/8 [==============================] - 1s 146ms/step - loss: 1.2201\n",
      "8/8 [==============================] - 1s 151ms/step - loss: 1.2089\n",
      "8/8 [==============================] - 1s 137ms/step - loss: 1.2192\n",
      "8/8 [==============================] - 1s 145ms/step - loss: 1.2325\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 1.2151\n",
      "8/8 [==============================] - 1s 142ms/step - loss: 1.1890\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.2383\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.1920\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.2055\n",
      "8/8 [==============================] - 5s 12ms/step\n",
      "8/8 [==============================] - 7s 15ms/step\n",
      "8/8 [==============================] - 7s 21ms/step\n",
      "8/8 [==============================] - 7s 22ms/step\n",
      "8/8 [==============================] - 7s 21ms/step\n",
      "8/8 [==============================] - 7s 15ms/step\n",
      "8/8 [==============================] - 7s 24ms/step\n",
      "8/8 [==============================] - 7s 21ms/step\n",
      "8/8 [==============================] - 7s 20ms/step\n",
      "8/8 [==============================] - 7s 18ms/step\n",
      "8/8 [==============================] - 7s 17ms/step\n",
      "8/8 [==============================] - 7s 19ms/step\n",
      "8/8 [==============================] - 7s 11ms/step\n",
      "8/8 [==============================] - 7s 17ms/step\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.3480\n",
      "8/8 [==============================] - 5s 6ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4784\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.6303\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.6760\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.5248\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.6760\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2.4895\n",
      "8/8 [==============================] - 0s 6ms/steposs: 2.21\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.4907\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5086\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.5776\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.5699\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1.5886\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1.5617\n",
      "8/8 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 2ms/step\n",
      "8/8 [==============================] - 1s 2ms/step\n",
      "8/8 [==============================] - 1s 2ms/step\n",
      "WARNING:tensorflow:5 out of the last 61 calls to <function Model.make_test_function.<locals>.test_function at 0x000001A2A1F4FA30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 62 calls to <function Model.make_test_function.<locals>.test_function at 0x000001A29676CC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.1891\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.1622\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.1442\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.1989\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.1502\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.5825\n",
      "8/8 [==============================] - 1s 2ms/step\n",
      "8/8 [==============================] - 1s 2ms/step\n",
      "8/8 [==============================] - 1s 2ms/step\n",
      "8/8 [==============================] - 1s 3ms/step\n",
      "8/8 [==============================] - 1s 3ms/step\n",
      "8/8 [==============================] - 1s 3ms/step\n",
      "8/8 [==============================] - 1s 106ms/step - loss: 7.1826\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 19.5512\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 9.2288\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 9.4145\n",
      "8/8 [==============================] - 1s 16ms/step\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.3574\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.2930\n",
      "8/8 [==============================] - 1s 49ms/step\n",
      "8/8 [==============================] - 1s 70ms/stepss: 1.181\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.4187\n",
      "8/8 [==============================] - 1s 39ms/step\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.3459\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 5.1093\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 5.1360\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 5.2280\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 5.1720\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 4.7920\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 5.1720\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 5.1400\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 5.1720\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 5.1380\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 5.1400\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 5.1075\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 5.0371\n",
      "8/8 [==============================] - 2s 92ms/stepss: 5.6708\n",
      "8/8 [==============================] - 2s 101ms/steps: 5.158\n",
      "8/8 [==============================] - 1s 104ms/step - loss: 5.0378\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 4.7920\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 5.1400\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 5.2280\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 4.7920\n",
      "8/8 [==============================] - 1s 130ms/step - loss: 5.1720\n",
      "8/8 [==============================] - 1s 132ms/step - loss: 5.2280\n",
      "8/8 [==============================] - 2s 128ms/steps: 4.85\n",
      "8/8 [==============================] - 1s 131ms/step - loss: 5.1400\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 4.7920\n",
      "8/8 [==============================] - 1s 134ms/step - loss: 4.7049\n",
      "8/8 [==============================] - 1s 116ms/step - loss: 5.2280\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 4.7078\n",
      "8/8 [==============================] - 2s 93ms/step\n",
      "8/8 [==============================] - 2s 72ms/step\n",
      "8/8 [==============================] - 2s 48ms/step\n",
      "8/8 [==============================] - 2s 100ms/step\n",
      "8/8 [==============================] - 2s 102ms/step\n",
      "8/8 [==============================] - 2s 115ms/step\n",
      "8/8 [==============================] - 2s 91ms/step\n",
      "8/8 [==============================] - 2s 95ms/step\n",
      "8/8 [==============================] - 2s 109ms/step\n",
      "8/8 [==============================] - 2s 94ms/step\n",
      "8/8 [==============================] - 2s 103ms/step\n",
      "8/8 [==============================] - 2s 84ms/step\n",
      "8/8 [==============================] - 2s 42ms/step\n",
      "8/8 [==============================] - 2s 56ms/step\n",
      "8/8 [==============================] - 2s 73ms/step\n",
      "8/8 [==============================] - 2s 81ms/step\n",
      "8/8 [==============================] - 2s 96ms/step\n",
      "8/8 [==============================] - 2s 90ms/step\n",
      "8/8 [==============================] - 2s 93ms/step\n",
      "8/8 [==============================] - 2s 87ms/step\n",
      "8/8 [==============================] - 2s 77ms/step\n",
      "8/8 [==============================] - 2s 74ms/step\n",
      "8/8 [==============================] - 2s 55ms/step\n",
      "8/8 [==============================] - 2s 110ms/step\n",
      "8/8 [==============================] - 2s 63ms/step\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.2222\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1.2456\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.7222\n",
      "8/8 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 1s 4ms/step\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 4.0497\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 4.1054\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 4.0332\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 3.6677\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 4.0974\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.0104\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.0286\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 4.0878\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 4.0475\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.9852\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 4.0422\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 3.6463\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 3.6431\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 3.9694\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 4.0078\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 3.6449\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 4.1029\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 4.1068\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3.6534\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 4.0169\n",
      "8/8 [==============================] - 6s 7ms/step\n",
      "8/8 [==============================] - 4s 6ms/step\n",
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 4s 4ms/step\n",
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 4s 6ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 3s 11ms/step\n",
      "8/8 [==============================] - 3s 7ms/step\n",
      "8/8 [==============================] - 3s 10ms/step\n",
      "8/8 [==============================] - 3s 10ms/step\n",
      "8/8 [==============================] - 3s 12ms/step\n",
      "8/8 [==============================] - 3s 6ms/step\n",
      "8/8 [==============================] - 3s 11ms/step\n",
      "8/8 [==============================] - 2s 8ms/step\n",
      "8/8 [==============================] - 3s 6ms/step\n",
      "8/8 [==============================] - 3s 12ms/step\n",
      "8/8 [==============================] - 2s 4ms/step\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 3.4329\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 3.1458\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4338\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 3.6233\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3.4373\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 3.5944\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 3.5972\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 3.5080\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 3.5281\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3.1136\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 3.5187\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 3.1761\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.1244\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 3.0551\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 3.5076\n",
      "8/8 [==============================] - 1s 132ms/step - loss: 3.4534\n",
      "8/8 [==============================] - 1s 9ms/step - loss: 3.4643\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 3.5887\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 3.4997\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3.4711\n",
      "8/8 [==============================] - 5s 4ms/step\n",
      "8/8 [==============================] - 5s 5ms/step\n",
      "8/8 [==============================] - 5s 5ms/step\n",
      "8/8 [==============================] - 3s 3ms/step\n",
      "8/8 [==============================] - 4s 6ms/step\n",
      "8/8 [==============================] - 4s 13ms/step\n",
      "8/8 [==============================] - 3s 8ms/step\n",
      "8/8 [==============================] - 4s 18ms/step\n",
      "8/8 [==============================] - 4s 13ms/step\n",
      "8/8 [==============================] - 4s 12ms/step\n",
      "8/8 [==============================] - 4s 11ms/step\n",
      "8/8 [==============================] - 4s 7ms/step\n",
      "8/8 [==============================] - 3s 11ms/step\n",
      "8/8 [==============================] - 3s 9ms/step\n",
      "8/8 [==============================] - 4s 9ms/step\n",
      "8/8 [==============================] - 3s 6ms/step\n",
      "8/8 [==============================] - 4s 3ms/step\n",
      "8/8 [==============================] - 3s 3ms/step\n",
      "8/8 [==============================] - 4s 3ms/step\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.4125\n",
      "8/8 [==============================] - 1s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from model_data_definitions import *\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to generate cross validation dataset\n",
    "def generate_cross_validation_dataset(data, num_folds):\n",
    "    X, y = data.drop('target', axis=1).values, data['target'].values\n",
    "    \n",
    "    dataset_list = []\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    fold = 0\n",
    "\n",
    "    # Splitting data into training and testing set for each fold in the cross-validation \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold += 1\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        dataset_list.append((X_train, y_train, X_test, y_test , fold))\n",
    "    \n",
    "    return dataset_list\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_evaluate_model(model_tuple, fold_data, epoch_number, dataset_name):\n",
    "    \n",
    "    model, name = model_tuple\n",
    "    X_train, y_train, X_test, y_test , fold = fold_data\n",
    "    \n",
    "    # Training the model \n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epoch_number,\n",
    "                        verbose=0,\n",
    "                        validation_data=(X_test,y_test))\n",
    "\n",
    "     # Evaluating the trained model on test data \n",
    "    loss = model.evaluate(X_test,y_test)\n",
    "\n",
    "     # Making predictions on the test data \n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "     # Calculate metrics \n",
    "    r_squared_value=r2_score(y_true=y_test,y_pred=predictions)\n",
    "    test_error=mean_squared_error(y_true=y_test,y_pred=predictions)\n",
    "\n",
    "    results = {\n",
    "        'model': name,\n",
    "        'fold': fold,\n",
    "        'train_history': history.history['loss'],\n",
    "        'val_history': history.history['val_loss'],\n",
    "        'loss': loss,\n",
    "        'r_squared_value': r_squared_value,\n",
    "        'test_error': test_error}\n",
    "\n",
    "    # Save results to numpy file\n",
    "    if not os.path.exists('aggregate_results'):\n",
    "        os.makedirs('aggregate_results')\n",
    "\n",
    "    np.save(f'aggregate_results/{dataset_name}-{name}-fold{fold}.npy', results)\n",
    "\n",
    "# Function to evaluate models in parallel\n",
    "def evaluate_models_parallel(fold_data, dataset_name, epoch_number):\n",
    "    \n",
    "    models = initialize_all_models(fold_data[0].shape[1], seed_val=fold_data[4])\n",
    "    compile_models(models)\n",
    "\n",
    "     # Training and evaluating all models in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(train_evaluate_model, model, fold_data, epoch_number, dataset_name): model for model in models}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# Function to evaluate all folds in parallel\n",
    "def evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number):\n",
    "    \n",
    "     # Evaluating all folds in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(evaluate_models_parallel, fold_data, dataset_name, epoch_number): fold_data for fold_data in kfold_datasets}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# New function to retrieve all datasets and their names and feed it to relevant functions with a for loop.\n",
    "def retrieve_datasets_and_run_evaluations(num_folds=5, epoch_number=100):\n",
    "    # Fetching data \n",
    "    filtered_datasets_metadata, datasets = fetch_return_filtered_pmlb_data_sets()\n",
    "\n",
    "    for dataset, row in zip(datasets, filtered_datasets_metadata.iterrows()):\n",
    "        dataset_name = row[1]['dataset']\n",
    "        kfold_datasets = generate_cross_validation_dataset(dataset, num_folds)\n",
    "        evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number)\n",
    "\n",
    "# Call the new function \n",
    "retrieve_datasets_and_run_evaluations(num_folds=4, epoch_number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d2a383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 97ms/step - loss: 2.3469\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 3.2009\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 3.5280\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 1.8113\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.5200\n",
      "8/8 [==============================] - 1s 15ms/step\n",
      "8/8 [==============================] - 1s 22ms/stepss: 0.48\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.4878\n",
      "8/8 [==============================] - 1s 96ms/stepss: 0.51\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.4854\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 0.5112\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 2.7320\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 2.5710\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 2.5701\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.8040\n",
      "8/8 [==============================] - 2s 64ms/step\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 2.6094\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 2.5233\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 2.6304\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 2.7063\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 2.4893\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 2.7583\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 2.6760\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 2.7360\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.7320\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 2.6753\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 2.8040\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 2.6760\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 2.7360\n",
      "8/8 [==============================] - 1s 81ms/step\n",
      "8/8 [==============================] - 1s 68ms/stepss: 2.504\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 2.5594\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 2.6341\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 2.4905\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 2.5252\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 2.5224\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 2.5886\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.5880\n",
      "8/8 [==============================] - 2s 75ms/step\n",
      "8/8 [==============================] - 2s 57ms/step\n",
      "8/8 [==============================] - 2s 47ms/step\n",
      "8/8 [==============================] - 1s 23ms/step\n",
      "8/8 [==============================] - 2s 58ms/step\n",
      "8/8 [==============================] - 2s 74ms/step\n",
      "8/8 [==============================] - 2s 88ms/step\n",
      "8/8 [==============================] - 2s 53ms/step\n",
      "8/8 [==============================] - 1s 34ms/step\n",
      "8/8 [==============================] - 1s 56ms/step\n",
      "8/8 [==============================] - 2s 66ms/step\n",
      "8/8 [==============================] - 2s 61ms/step\n",
      "8/8 [==============================] - 2s 44ms/step\n",
      "6/8 [=====================>........] - ETA: 0sWARNING:tensorflow:5 out of the last 22 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002B3C2347A30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "8/8 [==============================] - 2s 42ms/step\n",
      "8/8 [==============================] - 2s 36ms/step\n",
      "8/8 [==============================] - 2s 34ms/step\n",
      "8/8 [==============================] - 2s 73ms/step\n",
      "8/8 [==============================] - 2s 50ms/step\n",
      "8/8 [==============================] - 2s 43ms/step\n",
      "8/8 [==============================] - 2s 37ms/step\n",
      "8/8 [==============================] - 2s 62ms/step\n",
      "8/8 [==============================] - 2s 60ms/step\n",
      "8/8 [==============================] - 1s 47ms/step\n",
      "8/8 [==============================] - 2s 55ms/step\n",
      "8/8 [==============================] - 2s 58ms/step\n",
      "8/8 [==============================] - 2s 59ms/step\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5796\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5248\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5776\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6693\n",
      "8/8 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 1s 4ms/step\n",
      "8/8 [==============================] - 1s 7ms/step\n",
      "8/8 [==============================] - 1s 9ms/step\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.5832\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.8909WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x000002B3FF6BB9A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x000002B3C4A0E830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.6406\n",
      "8/8 [==============================] - 1s 58ms/step - loss: 1.6960\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.6368\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 1.5874\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 1.5765\n",
      "8/8 [==============================] - 1s 133ms/step - loss: 1.6549\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.6277\n",
      "8/8 [==============================] - 1s 129ms/step - loss: 1.6128\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 1.5615\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 1.6398\n",
      "8/8 [==============================] - 1s 112ms/step - loss: 1.6345\n",
      "8/8 [==============================] - 1s 129ms/step - loss: 1.6265\n",
      "8/8 [==============================] - 1s 141ms/step - loss: 1.6368\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 1.5687\n",
      "8/8 [==============================] - 1s 111ms/step - loss: 1.6828\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 1.6962\n",
      "8/8 [==============================] - 1s 112ms/step - loss: 1.6876\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 1.7107\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.6396\n",
      "8/8 [==============================] - 6s 28ms/step\n",
      "8/8 [==============================] - 5s 15ms/step\n",
      "8/8 [==============================] - 5s 4ms/step\n",
      "8/8 [==============================] - 5s 4ms/step\n",
      "8/8 [==============================] - 3s 4ms/step\n",
      "8/8 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 6ms/step\n",
      "1/8 [==>...........................] - ETA: 19sWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002B414B6C9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "8/8 [==============================] - 3s 18ms/step\n",
      "8/8 [==============================] - 3s 14ms/step\n",
      "8/8 [==============================] - 3s 21ms/step\n",
      "8/8 [==============================] - 3s 21ms/step\n",
      "8/8 [==============================] - 3s 15ms/step\n",
      "8/8 [==============================] - 3s 7ms/step\n",
      "8/8 [==============================] - 3s 22ms/step\n",
      "8/8 [==============================] - 3s 28ms/step\n",
      "8/8 [==============================] - 3s 19ms/step\n",
      "8/8 [==============================] - 3s 15ms/step\n",
      "8/8 [==============================] - 3s 12ms/step\n",
      "8/8 [==============================] - 3s 13ms/step\n",
      "8/8 [==============================] - 3s 12ms/step\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.1950\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1954\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.2021\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.2210\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.1725\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.1938\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.2385\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.2422\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.2162\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.1544\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.1704\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2496\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.1937\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.1858\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.2242\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1.2038\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.2290\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1619\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.1886\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2020\n",
      "8/8 [==============================] - 4s 4ms/step\n",
      "8/8 [==============================] - 4s 7ms/step\n",
      "8/8 [==============================] - 4s 11ms/step\n",
      "8/8 [==============================] - 4s 13ms/step\n",
      "8/8 [==============================] - 4s 10ms/step\n",
      "8/8 [==============================] - 4s 11ms/step\n",
      "8/8 [==============================] - 4s 10ms/step\n",
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 4s 7ms/step\n",
      "8/8 [==============================] - 4s 6ms/step\n",
      "8/8 [==============================] - 4s 6ms/step\n",
      "8/8 [==============================] - 4s 6ms/step\n",
      "8/8 [==============================] - 4s 6ms/step\n",
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 4s 4ms/step\n",
      "8/8 [==============================] - 4s 2ms/step\n",
      "8/8 [==============================] - 4s 3ms/step\n",
      "8/8 [==============================] - 1s 1ms/step\n",
      "8/8 [==============================] - 1s 1ms/step\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 19.5510\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 9.4272\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 7.1998\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.3766\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 9.2343\n",
      "8/8 [==============================] - 1s 24ms/stepss: 1.22\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.3586\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.3778\n",
      "8/8 [==============================] - 1s 50ms/step\n",
      "8/8 [==============================] - 1s 60ms/stepss: 5.25\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 5.2280\n",
      "8/8 [==============================] - 1s 38ms/step\n",
      "8/8 [==============================] - 1s 59ms/stepss: 1.27\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.2749\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 5.1400\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 4.7920\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 4.7081\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 5.1400\n",
      "8/8 [==============================] - 1s 114ms/step - loss: 5.1089\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 5.0379\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 4.7051\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 5.2280\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 5.1382\n",
      "8/8 [==============================] - 2s 102ms/step\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 5.1400\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 5.0369\n",
      "8/8 [==============================] - 1s 109ms/step - loss: 5.2280\n",
      "8/8 [==============================] - 1s 104ms/step - loss: 5.1720\n",
      "8/8 [==============================] - 1s 129ms/step - loss: 4.7920\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 4.7920\n",
      "8/8 [==============================] - 1s 115ms/step - loss: 5.1077\n",
      "8/8 [==============================] - 1s 106ms/step - loss: 5.1400\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 5.2280\n",
      "8/8 [==============================] - 2s 110ms/steps: 5.\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 5.1362\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 5.1720\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 5.1720\n",
      "8/8 [==============================] - 2s 56ms/stepss: 5.10\n",
      "8/8 [==============================] - 1s 39ms/stepss: 5.09\n",
      "8/8 [==============================] - 2s 59ms/stepss: 5.1\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 5.1720\n",
      "8/8 [==============================] - 2s 58ms/step\n",
      "8/8 [==============================] - 2s 95ms/step\n",
      "8/8 [==============================] - 2s 60ms/step\n",
      "8/8 [==============================] - 2s 42ms/step\n",
      "8/8 [==============================] - 2s 63ms/step\n",
      "8/8 [==============================] - 2s 39ms/step\n",
      "8/8 [==============================] - 2s 119ms/step\n",
      "8/8 [==============================] - 2s 135ms/step\n",
      "8/8 [==============================] - 2s 147ms/step\n",
      "8/8 [==============================] - 2s 159ms/step\n",
      "8/8 [==============================] - 2s 148ms/step\n",
      "8/8 [==============================] - 2s 138ms/step\n",
      "8/8 [==============================] - 2s 144ms/step\n",
      "8/8 [==============================] - 2s 145ms/step\n",
      "8/8 [==============================] - 2s 137ms/step\n",
      "8/8 [==============================] - 2s 129ms/step\n",
      "8/8 [==============================] - 2s 134ms/step\n",
      "8/8 [==============================] - 2s 104ms/step\n",
      "8/8 [==============================] - 2s 116ms/step\n",
      "8/8 [==============================] - 2s 132ms/step\n",
      "8/8 [==============================] - 1s 39ms/step\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.2936\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 1.2235\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.4539\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.7254\n",
      "8/8 [==============================] - 1s 10ms/step\n",
      "8/8 [==============================] - 1s 8ms/step\n",
      "8/8 [==============================] - 1s 8ms/step\n",
      "8/8 [==============================] - 1s 6ms/step\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 4.0887\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 4.0167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 74ms/step - loss: 4.1068\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 4.0496\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 4.0121\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 4.0085\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 3.6432\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 3.9860\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 3.6450\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 4.0431\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 3.9693\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 3.6658\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 4.1030\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 4.1050\n",
      "8/8 [==============================] - 1s 114ms/step - loss: 3.6526\n",
      "8/8 [==============================] - 1s 129ms/step - loss: 4.0308\n",
      "8/8 [==============================] - 1s 129ms/step - loss: 4.0971\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 4.0328\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.6467\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 4.0462\n",
      "8/8 [==============================] - 4s 13ms/step\n",
      "8/8 [==============================] - 4s 14ms/step\n",
      "8/8 [==============================] - 4s 6ms/step\n",
      "8/8 [==============================] - 4s 8ms/step\n",
      "8/8 [==============================] - 4s 7ms/step\n",
      "8/8 [==============================] - 3s 6ms/step\n",
      "8/8 [==============================] - 3s 7ms/step\n",
      "8/8 [==============================] - 3s 9ms/step\n",
      "8/8 [==============================] - 4s 16ms/step\n",
      "8/8 [==============================] - 3s 19ms/step\n",
      "8/8 [==============================] - 3s 19ms/step\n",
      "8/8 [==============================] - 3s 18ms/step\n",
      "8/8 [==============================] - 3s 23ms/step\n",
      "8/8 [==============================] - 3s 12ms/step\n",
      "8/8 [==============================] - 3s 22ms/step\n",
      "8/8 [==============================] - 3s 12ms/step\n",
      "8/8 [==============================] - 3s 21ms/step\n",
      "8/8 [==============================] - 3s 13ms/step\n",
      "8/8 [==============================] - 3s 11ms/step\n",
      "8/8 [==============================] - 2s 9ms/step\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 3.5345\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 3.5571\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 3.4992\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3.2267\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 3.5536\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 3.4367\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 3.5900\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 3.4539\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 3.6142\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 3.5629\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 3.5329\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 3.4923\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 3.4337\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3.4611\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.1683\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 3.1878\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3.4776\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3.2019\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 3.4418\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 3.1755\n",
      "8/8 [==============================] - 4s 15ms/step\n",
      "8/8 [==============================] - 4s 17ms/step\n",
      "8/8 [==============================] - 4s 13ms/step\n",
      "8/8 [==============================] - 4s 12ms/step\n",
      "8/8 [==============================] - 4s 19ms/step\n",
      "8/8 [==============================] - 4s 19ms/step\n",
      "8/8 [==============================] - 4s 13ms/step\n",
      "8/8 [==============================] - 4s 14ms/step\n",
      "8/8 [==============================] - 4s 11ms/step\n",
      "8/8 [==============================] - 4s 14ms/step\n",
      "8/8 [==============================] - 4s 11ms/step\n",
      "8/8 [==============================] - 4s 10ms/step\n",
      "8/8 [==============================] - 4s 12ms/step\n",
      "8/8 [==============================] - 4s 7ms/step\n",
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 4s 4ms/step\n",
      "8/8 [==============================] - 4s 5ms/step\n",
      "8/8 [==============================] - 4s 4ms/step\n",
      "8/8 [==============================] - 3s 2ms/step\n",
      "8/8 [==============================] - 3s 3ms/step\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7920\n",
      "8/8 [==============================] - 0s 991us/step\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from model_data_definitions import *\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to generate cross validation dataset\n",
    "def generate_cross_validation_dataset(data, num_folds):\n",
    "    X, y = data.drop('target', axis=1).values, data['target'].values\n",
    "    \n",
    "    dataset_list = []\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    fold = 0\n",
    "\n",
    "    # Splitting data into training and testing set for each fold in the cross-validation \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold += 1\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        dataset_list.append((X_train, y_train, X_test, y_test , fold))\n",
    "    \n",
    "    return dataset_list\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_evaluate_model(model_tuple, fold_data, epoch_number, dataset_name,num_folds):\n",
    "    \n",
    "    model, name = model_tuple\n",
    "    X_train, y_train, X_test, y_test , fold = fold_data\n",
    "    \n",
    "    # Training the model \n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epoch_number,\n",
    "                        verbose=0,\n",
    "                        validation_data=(X_test,y_test))\n",
    "\n",
    "     # Evaluating the trained model on test data \n",
    "    loss = model.evaluate(X_test,y_test)\n",
    "\n",
    "     # Making predictions on the test data \n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "     # Calculate metrics \n",
    "    r_squared_value=r2_score(y_true=y_test,y_pred=predictions)\n",
    "    test_error=mean_squared_error(y_true=y_test,y_pred=predictions)\n",
    "\n",
    "    results = {\n",
    "        'model': name,\n",
    "        'fold': fold,\n",
    "        'train_history': history.history['loss'],\n",
    "        'val_history': history.history['val_loss'],\n",
    "        'loss': loss,\n",
    "        'r_squared_value': r_squared_value,\n",
    "        'test_error': test_error}\n",
    "\n",
    "    # Save results to numpy file\n",
    "    if not os.path.exists('aggregate_results'):\n",
    "        os.makedirs('aggregate_results')\n",
    "\n",
    "    np.save(f'aggregate_results/{dataset_name}-{name}-fold-{fold}-of-{num_folds}.npy', results)\n",
    "\n",
    "# Function to evaluate models in parallel\n",
    "def evaluate_models_parallel(fold_data, dataset_name, epoch_number,num_folds):\n",
    "    \n",
    "    models = initialize_all_models(fold_data[0].shape[1], seed_val=fold_data[4])\n",
    "    compile_models(models)\n",
    "\n",
    "     # Training and evaluating all models in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(train_evaluate_model, model, fold_data, epoch_number, dataset_name,num_folds): model for model in models}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# Function to evaluate all folds in parallel\n",
    "def evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number,num_folds):\n",
    "    \n",
    "     # Evaluating all folds in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(evaluate_models_parallel, fold_data, dataset_name, epoch_number,num_folds): fold_data for fold_data in kfold_datasets}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# New function to retrieve all datasets and their names and feed it to relevant functions with a for loop.\n",
    "def retrieve_datasets_and_run_evaluations(num_folds=5, epoch_number=100):\n",
    "    # Fetching data \n",
    "    filtered_datasets_metadata, datasets = fetch_return_filtered_pmlb_data_sets()\n",
    "\n",
    "    for dataset, row in zip(datasets, filtered_datasets_metadata.iterrows()):\n",
    "        dataset_name = row[1]['dataset']\n",
    "        kfold_datasets = generate_cross_validation_dataset(dataset, num_folds)\n",
    "        evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number,num_folds)\n",
    "\n",
    "# Call the new function \n",
    "retrieve_datasets_and_run_evaluations(num_folds=4, epoch_number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d886e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment took 1.6836485862731934 seconds to complete.\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from model_data_definitions import *\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "# Function to generate cross validation dataset\n",
    "def generate_cross_validation_dataset(data, num_folds):\n",
    "    X, y = data.drop('target', axis=1).values, data['target'].values\n",
    "    \n",
    "    dataset_list = []\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    fold = 0\n",
    "\n",
    "    # Splitting data into training and testing set for each fold in the cross-validation \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold += 1\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        dataset_list.append((X_train, y_train, X_test, y_test , fold))\n",
    "    \n",
    "    return dataset_list\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_evaluate_model(model_tuple, fold_data, epoch_number, dataset_name,num_folds):\n",
    "    \n",
    "    model, name = model_tuple\n",
    "    X_train, y_train, X_test, y_test , fold = fold_data\n",
    "    \n",
    "    # Training the model \n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epoch_number,\n",
    "                        verbose=0,\n",
    "                        validation_data=(X_test,y_test))\n",
    "\n",
    "     # Evaluating the trained model on test data \n",
    "    loss = model.evaluate(X_test,y_test)\n",
    "\n",
    "     # Making predictions on the test data \n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "     # Calculate metrics \n",
    "    r_squared_value=r2_score(y_true=y_test,y_pred=predictions)\n",
    "    test_error=mean_squared_error(y_true=y_test,y_pred=predictions)\n",
    "\n",
    "    results = {\n",
    "        'model': name,\n",
    "        'fold': fold,\n",
    "        'train_history': history.history['loss'],\n",
    "        'val_history': history.history['val_loss'],\n",
    "        'loss': loss,\n",
    "        'r_squared_value': r_squared_value,\n",
    "        'test_error': test_error}\n",
    "\n",
    "    # Save results to numpy file\n",
    "    if not os.path.exists('aggregate_results'):\n",
    "        os.makedirs('aggregate_results')\n",
    "\n",
    "    np.save(f'aggregate_results/{dataset_name}-{name}-fold-{fold}-of-{num_folds}.npy', results)\n",
    "\n",
    "# Function to evaluate models in parallel\n",
    "def evaluate_models_parallel(fold_data, dataset_name, epoch_number,num_folds):\n",
    "    \n",
    "    models = initialize_all_models(fold_data[0].shape[1], seed_val=fold_data[4])\n",
    "    compile_models(models)\n",
    "\n",
    "     # Training and evaluating all models in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(train_evaluate_model, model, fold_data, epoch_number, dataset_name,num_folds): model for model in models}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# Function to evaluate all folds in parallel\n",
    "def evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number,num_folds):\n",
    "    \n",
    "     # Evaluating all folds in parallel using ThreadPoolExecutor \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(evaluate_models_parallel, fold_data, dataset_name, epoch_number,num_folds): fold_data for fold_data in kfold_datasets}\n",
    "        for future in futures:\n",
    "            future.result()  # Just to make sure all tasks are finished\n",
    "\n",
    "# New function to retrieve all datasets and their names and feed it to relevant functions with a for loop.\n",
    "def retrieve_datasets_and_run_evaluations(num_folds=5, epoch_number=100):\n",
    "    # Fetching data \n",
    "    filtered_datasets_metadata, datasets = fetch_return_filtered_pmlb_data_sets()\n",
    "\n",
    "    for dataset, row in zip(datasets, filtered_datasets_metadata.iterrows()):\n",
    "        dataset_name = row[1]['dataset']\n",
    "        kfold_datasets = generate_cross_validation_dataset(dataset, num_folds)\n",
    "        evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number,num_folds)\n",
    "\n",
    "# Call the new function \n",
    "start_time = time.time()\n",
    "retrieve_datasets_and_run_evaluations(num_folds=4, epoch_number=2, parallel=True)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The experiment took {elapsed_time} seconds to complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7aaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experiment took 162.88905668258667 seconds to complete.\n",
    "# The experiment took 173.05344152450562 seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819bc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcafe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5dcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function to retrieve all datasets and their names and feed it to relevant functions with a for loop.\n",
    "def retrieve_datasets_and_run_evaluations(num_folds=5, epoch_number=100, parallel=True):\n",
    "    # Fetching data \n",
    "    filtered_datasets_metadata, datasets = fetch_return_filtered_pmlb_data_sets()\n",
    "\n",
    "    if parallel:\n",
    "        # Use ThreadPoolExecutor to spawn separate threads for each dataset.\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = {executor.submit(evaluate_all_folds_parallel, generate_cross_validation_dataset(dataset, num_folds), row[1]['dataset'], epoch_number,num_folds): dataset for dataset, row in zip(datasets, filtered_datasets_metadata.iterrows())}\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "    else:\n",
    "        # Use a simple for loop to process each dataset sequentially.\n",
    "        for dataset, row in zip(datasets, filtered_datasets_metadata.iterrows()):\n",
    "            dataset_name = row[1]['dataset']\n",
    "            kfold_datasets = generate_cross_validation_dataset(dataset, num_folds)\n",
    "            evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number,num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function to retrieve all datasets and their names and feed it to relevant functions with a for loop.\n",
    "def retrieve_datasets_and_run_evaluations(num_folds=5, epoch_number=100, parallel=True):\n",
    "    # Fetching data \n",
    "    filtered_datasets_metadata, datasets = fetch_return_filtered_pmlb_data_sets()\n",
    "\n",
    "    if parallel:\n",
    "        # Use ProcessPoolExecutor to spawn separate processes for each dataset.\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            executor.map(evaluate_all_folds_parallel, \n",
    "                         [generate_cross_validation_dataset(dataset, num_folds) for dataset in datasets], \n",
    "                         [row[1]['dataset'] for row in filtered_datasets_metadata.iterrows()], \n",
    "                         [epoch_number]*len(datasets), \n",
    "                         [num_folds]*len(datasets))\n",
    "    else:\n",
    "        # Use a simple for loop to process each dataset sequentially.\n",
    "        for dataset, row in zip(datasets, filtered_datasets_metadata.iterrows()):\n",
    "            dataset_name = row[1]['dataset']\n",
    "            kfold_datasets = generate_cross_validation_dataset(dataset, num_folds)\n",
    "            evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number,num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function to retrieve all datasets and their names and feed it to relevant functions with a for loop.\n",
    "def retrieve_datasets_and_run_evaluations(num_folds=5, epoch_number=100, parallel=True):\n",
    "    # Fetching data \n",
    "    filtered_datasets_metadata, datasets = fetch_return_filtered_pmlb_data_sets()\n",
    "\n",
    "    if parallel:\n",
    "        # Use ProcessPoolExecutor to spawn separate processes for each dataset.\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            all_experiments = []\n",
    "            for dataset, row in zip(datasets, filtered_datasets_metadata.iterrows()):\n",
    "                dataset_name = row[1]['dataset']\n",
    "                kfold_datasets = generate_cross_validation_dataset(dataset, num_folds)\n",
    "                experiment_params = (kfold_datasets, dataset_name, epoch_number,num_folds)\n",
    "                all_experiments.append(experiment_params)\n",
    "            \n",
    "            executor.map(evaluate_all_folds_parallel,*zip(*all_experiments))\n",
    "    else:\n",
    "        # Use a simple for loop to process each dataset sequentially.\n",
    "        for dataset, row in zip(datasets, filtered_datasets_metadata.iterrows()):\n",
    "            dataset_name = row[1]['dataset']\n",
    "            kfold_datasets = generate_cross_validation_dataset(dataset, num_folds)\n",
    "            evaluate_all_folds_parallel(kfold_datasets, dataset_name, epoch_number,num_folds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
